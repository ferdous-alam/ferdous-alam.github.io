<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Transformer implementation deconstructed | Ferdous Alam </title> <meta name="author" content="Ferdous Alam"> <meta name="description" content="A cheat sheet for implementing transformers."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ferdous-alam.github.io/blog/2022/formatting-and-links/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ferdous</span> Alam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/codes/">codes </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Transformer implementation deconstructed</h1> <p class="post-meta"> Created in August 21, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI,</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformer</a>     ·   <a href="/blog/category/generative-models"> <i class="fa-solid fa-tag fa-sm"></i> generative-models</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p><strong><em>NOTE:</em></strong> Code implementation can be found at this <a href="https://github.com/ferdous-alam/DeconstructedML/tree/master/Deconstructed_Deep_learning/Transformers" rel="external nofollow noopener" target="_blank">repo</a>.</p> </blockquote> <h2 id="introduction">Introduction</h2> <p>Few symbols:</p> <table> <thead> <tr> <th>symbol</th> <th>tensor shape</th> <th>description</th> </tr> </thead> <tbody> <tr> <td>$B$</td> <td>-</td> <td>batch size</td> </tr> <tr> <td>$l_z$</td> <td>-</td> <td>context/source sequence length</td> </tr> <tr> <td>$l_x$</td> <td>-</td> <td>primary/target sequence length</td> </tr> <tr> <td>$d_\text{model}$</td> <td>-</td> <td>input dimension</td> </tr> <tr> <td>$d_k$</td> <td>-</td> <td>dimension of query/key embedding</td> </tr> <tr> <td>$d_v$</td> <td>-</td> <td>dimension of value embedding</td> </tr> <tr> <td>$B$</td> <td>-</td> <td>batch size</td> </tr> <tr> <td>$h$</td> <td>-</td> <td>number of heads</td> </tr> <tr> <td>$LN$</td> <td>-</td> <td>layer norm</td> </tr> <tr> <td>$FFN$</td> <td>-</td> <td>feed forward network</td> </tr> <tr> <td>$\mathbf{Z}$</td> <td>$(B, l_z, d_\text{model})$</td> <td>context/source sequence</td> </tr> <tr> <td>$\mathbf{X}$</td> <td>$(B, l_x, d_\text{model})$</td> <td>primary/target sequence</td> </tr> <tr> <td>$\mathbf{M}_{\mathbf{zz}}$</td> <td>$(B, l_z, l_z)$</td> <td>source mask</td> </tr> <tr> <td>$\mathbf{M}_{\mathbf{xx}}$</td> <td>$(B, l_x, l_x)$</td> <td>target mask</td> </tr> <tr> <td>$\mathbf{M}_{\mathbf{xz}}$</td> <td>$(l_z, l_z)$</td> <td>memory mask</td> </tr> <tr> <td>$L_{enc}$</td> <td>-</td> <td>number of encoder layers</td> </tr> <tr> <td>$L_{dec}$</td> <td>-</td> <td>number of decoder layers</td> </tr> </tbody> </table> <p>We start with the originally proposed encoder-decoder (ED) transformer [@vaswani2017attention]. If we only use the encoder of the transformer, then it is similar to the BERT [@devlin2018bert] model and if we only use the decoder of the transformer then it is similar to the GPT model [@radford2018improving]. For clarity, we consider a batched source sequence data $\mathbf{Z}$ which consists of $B$ sequences. Each sequence is of length $l_z$. This means that the sequence consists of $l_z$ number of tokens or vector representation of some input.</p> \[\mathbf{Z} = \begin{bmatrix} \mathbf{z}^1_1 &amp; \mathbf{z}^1_2 &amp; \dots &amp; \mathbf{z}^1_{l_z} \\ \mathbf{z}^2_1 &amp; \mathbf{z}^2_2 &amp; \dots &amp; \mathbf{z}^2_{l_z} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ \mathbf{z}^{B}_1 &amp; \mathbf{z}^{B}_2 &amp; \dots &amp; \mathbf{z}^{B}_{l_z} \end{bmatrix}\] <p>where, \(\mathbf{z}_i \in \mathbb{R}^{1\times d_\text{model}}, \ \ \ \ \ i = 1, 2, \dots, l_z\)</p> <p>Similarly, we consider a batched target sequence data $\mathbf{X}$ that contain $B$ sequences in total. Each target sequence is of length $l_x$.</p> \[\mathbf{X} = \begin{bmatrix} \mathbf{x}^1_1 &amp; \mathbf{x}^1_2 &amp; \dots &amp; \mathbf{x}^1_{l_x} \\ \mathbf{x}^2_1 &amp; \mathbf{x}^2_2 &amp; \dots &amp; \mathbf{x}^2_{l_x} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ \mathbf{x}^{B}_1 &amp; \mathbf{x}^{B}_2 &amp; \dots &amp; \mathbf{x}^{B}_{l_x} \end{bmatrix}\] <p>where, \(\mathbf{x}_i \in \mathbb{R}^{1\times d_\text{model}}, \ \ \ \ \ i = 1, 2, \dots, l_x\)</p> <p>The goal is to learn a representation of the target sequence, $\mathbf{X}$, that utilizes multi-head attention to capture important correlation within the sequence. Finally, people use these representations for various downstream tasks i.e. machine translation [@vaswani2017attention], next word prediction [@yang2019xlnet], computer vision [@dosovitskiy2020image], reinforcement learning [@chen2021decision] etc. A wonderful paper from DeepMind describes the formal algorithms for transformers [@phuong2022formal] very neatly.</p> <h3 id="mask">Mask</h3> <p>Mask allows the transformer to decide which part of the output should the model see at each timestep.</p> \[\mathbf{M}_{\mathbf{x}\mathbf{z}} = \begin{bmatrix} \text{Mask}[\mathbf{x}_0, \mathbf{z}_0] &amp; \text{Mask}[\mathbf{x}_0, \mathbf{z}_1]&amp; \text{Mask}[\mathbf{x}_0, \mathbf{z}_2] &amp; \dots &amp;\text{Mask}[\mathbf{x}_0, \mathbf{z}_{l_z}] \\ \text{Mask}[\mathbf{x}_1, \mathbf{z}_0] &amp; \text{Mask}[\mathbf{x}_1, \mathbf{z}_1]&amp; \text{Mask}[\mathbf{x}_1, \mathbf{z}_2] &amp; \dots &amp; \text{Mask}[\mathbf{x}_1, \mathbf{z}_{l_z}] \\ \vdots &amp; \vdots&amp; \vdots &amp; \dots &amp; \vdots \\ \text{Mask}[\mathbf{x}_{l_x}, \mathbf{z}_0] &amp; \text{Mask}[\mathbf{x}_{l_x}, \mathbf{z}_1]&amp; \text{Mask}[\mathbf{x}_{l_x}, \mathbf{z}_2] &amp; \dots &amp; \text{Mask}[\mathbf{x}_{l_x}, \mathbf{z}_{l_z}] \end{bmatrix} \in \mathbb{R}^{l_x \times l_z}\] <p>For example, if we want the model to see the whole sequence to calculate attention while training,then we do not need to do any masking. This model deploys <strong>bidirectional</strong> attention. The whole sequence should be available at the same time. For a single $i$-th sequence, the mask would look like this \(\mathbf{M}_{\mathbf{x}\mathbf{z}} = \begin{bmatrix} 1&amp; 1&amp; 1 &amp; \dots &amp; 1 \\ 1&amp; 1&amp; 1 &amp; \dots &amp; 1 \\ \vdots &amp; \vdots&amp; \vdots &amp; \dots &amp; \vdots \\ 1&amp; 1&amp; 1 &amp; \dots &amp; 1 \end{bmatrix} \in \mathbb{R}^{l_x \times l_z}\)</p> <p>Similarly, for auto-regressive models, we want the model to calculate attention based on unseen outputs until that timestep. Hence, the mask would be \(\mathbf{M}_{\mathbf{x}\mathbf{z}} = \begin{bmatrix} 1&amp; 0&amp; 0 &amp; \dots &amp; 0 \\ 1&amp; 1&amp; 0 &amp; \dots &amp; 0 \\ \vdots &amp; \vdots&amp; \vdots &amp; \dots &amp; \vdots \\ 1&amp; 1&amp; 1 &amp; \dots &amp; 1 \end{bmatrix} \in \mathbb{R}^{l_x \times l_z}\)</p> <p>In summary, if the length of each sequence at timestep $t$ is $t_x$ and $t_z$ respectively, then we can express each mask as the following,</p> \[\text{Mask}[t_x, t_z] = \begin{cases} 1 \quad \quad \quad \quad \text{for bidirectional attention} \\ [[t_x \geq t_z]] \quad \text{for unidirectional attention} \end{cases}\] <p>For conveniece, we introduce two additional terms:</p> <p>1) self-masks, $\mathbf{M}<em>{\mathbf{x}\mathbf{x}}$ or $\mathbf{M}</em>{\mathbf{z}\mathbf{z}}$: When we want to mask the same sequence against itself, for example we would use this sort of masking in the encoder part 2) cross-masks, $\mathbf{M}_{\mathbf{x}\mathbf{z}}$: When we want to mask a target sequence against a source sequence, for example we would use this sort of masking in the decoder part</p> <h2 id="formal-algorithm">Formal algorithm</h2> <p>First we provide a pseudoode of the encoder-decoder transformer algorithm. The following pseudocode is a simplified version of the formal algorithm presented in this paper [@phuong2022formal]. Initially I wanted to include the original pseudocode from the paper. But it seems like handling a lot of notations while thinking of the implementation. So, I added some trivial abstraction on top of that so the implementation becomes more convenient. Also, each matrix in that paper is transposed which makes the batched implementation little bit difficult to understand. So, I made some required modifications. This may reduce the technical correctness of the pseudocode, but I think that can be thought of as a simplification for implementation purpose.</p> <h3 id="encoder-decoder-transformer">Encoder-decoder transformer</h3> <p>| Algorithm 1: Encoder Decoder Transformer| | :— | | input: $\mathbf{Z} \in \mathbb{R}^{l_z \times d_\text{model}}$, $\mathbf{X} \in \mathbb{R}^{l_x \times d_\text{model} }$, vector representations of context and primary sequence, $L_{enc}$, $L_{dec}$, number of encoder and decoder layers, EncoderLayer class, DecoderLayer class, source_mask, target_mask, memory_mask| | output: $\mathbf{X} \in \mathbb{R}^{l_x \times d_\text{model}}$, representation of primary sequence with multi-head attention which can be used for downstream applications| | 1 For $i = 1, 2, \dots, L_{enc}$| | 2 $\quad \mathbf{Z} \leftarrow \text{EncoderLayer}(\mathbf{Z}, \mathbf{M}<em>{\mathbf{zz}}: Optional)$ | | 3 $\text{memory} = \mathbf{Z}$| | 4 For $j = 1, 2, \dots, L</em>{dec}$| | 5 $\quad \mathbf{X} \leftarrow \text{DecoderLayer}(\mathbf{X}, \text{memory}, \mathbf{M}<em>{\mathbf{xx}}: Optional, \mathbf{M}</em>{\mathbf{zz}}: Optional, \mathbf{M}_{\mathbf{xz}}: Optional)$|</p> <blockquote> <p><strong><em>Implementation note:</em></strong> For efficiency, deepcopy of a single encoder and decoder layer can be performed $L_{enc}$ and $L_{dec}$ times</p> </blockquote> <p>The training procedure is fairly simple and basically same as other neural network models. To make a broaded sense, I am not including any NLP specific output from the model. So, the output from the model is what we are interested in. We will also need to supply the target so that loss can be caluculated using the output and the target values. Finally we perform gradient descent to minimize the loss.</p> <h3 id="training-procedure">Training procedure</h3> <p>| Algorithm 2: Training Transformer| | :—————— | | $\text{input: } \text{class EDTransformer, class loss_func, learning rate $\eta$}$ | | 1 $\text{for } i = 1, 2, \dots, N_\text{epochs}$| | 2 $\quad \text{for } (\mathbf{Z}, \mathbf{X}, \text{target}) \text{ in train_dataloader } \quad \quad \text{# typical data loader for training data}$| | 3 $\quad \quad output \leftarrow \text{EDTransformer}(\mathbf{Z}, \mathbf{X}, \mathbf{M}<em>{\mathbf{xx}}: Optional, \mathbf{M}</em>{\mathbf{zz}}: Optional, \mathbf{M}_{\mathbf{xz}}: Optional)$| | 4 $\quad \quad \mathcal{L}(\mathbf{\theta}) = \text{loss_func}(\text{output, target})$| | 5 $\quad \quad \theta \leftarrow \theta - \eta \cdot \nabla\mathcal{L}(\theta)$| | 6 $\text{return } \mathbf{\theta}$|</p> <h2 id="attention">Attention</h2> <p>Transformers use multi-head attention to learn the contextual information of a sequence.</p> <h3 id="multihead-attention">Multihead attention</h3> <p>The original $Q, K, V$ matrices are projected into $h$ smaller matrices of using parameter matrices $W_i^Q, W_i^K, W_i^V$. Then attention is calculated for all these smaller matrices and concatened again to calculate attention for the full size input.</p> <table> <thead> <tr> <th>argument</th> <th>tensor shape</th> </tr> </thead> <tbody> <tr> <td>query $Q$</td> <td>$(B, l_x, d_\text{model})$</td> </tr> <tr> <td>key, $K$</td> <td>$(B, l_z, d_\text{model})$</td> </tr> <tr> <td>value, $V$</td> <td>$(B, l_z, d_\text{model})$</td> </tr> <tr> <td>$\text{multi_attn}$</td> <td>$(B, l_x, d_\text{out})$</td> </tr> </tbody> </table> <p>Let’s recall the original definition of multi-head attention: \(\text{Multi-head attention, }\mathbf{Y}(Q, K, V) = [\mathbf{S}_1; \dots; \mathbf{S}_h]W^O\) where $\mathbf{S}_i$ is the $i$-th single head attention score.</p> <p>As we are dividing the original $Q, K, V$ matrices into smaller matrices, dimension of $Q, K, V$ must be divisible by the number of heads, $h$. This is one way to do that if we want to divide $Q, K, V$ into the same dimension of smaller matrices. \(d_k = d_v = d_\text{model} / h\) Alternatively, we can divide $Q, K, V$ into different dimensions of smaller matrices as long as they match the original dimension.</p> <table> <thead> <tr> <th>Parameters</th> <th>dimension</th> </tr> </thead> <tbody> <tr> <td>query projection FFN, $W^Q$</td> <td>$\mathbb{R}^{d_\text{model}\times d_k}$</td> </tr> <tr> <td>key projection FFN, $W^K$</td> <td>$\mathbb{R}^{d_\text{model}\times d_k}$</td> </tr> <tr> <td>value projection FFN, $W^V$</td> <td>$\mathbb{R}^{d_\text{out}\times d_v}$</td> </tr> <tr> <td>output projection FFN, $W^O$</td> <td>$\mathbb{R}^{hd_v\times d_\text{out}}$</td> </tr> </tbody> </table> <p>So, these parameter weight matrices help to project the original $Q, K, V$ into smaller $h$ number of $q, k, v$ matrices for multi-head purpose. \(\text{query projection, }W_i^Q: Q \rightarrow \mathbf{q}_i\) \(\text{key projection, }W_i^K: K \rightarrow \mathbf{k}_i\) \(\text{value projection, }W_i^V: V \rightarrow \mathbf{v}_i\) For efficient implementation we calculate the attention score for all heads simultaneously by reshaping the tensors. So, the shape of the smaller tensors end up being the following,</p> <table> <thead> <tr> <th>tensor</th> <th>expression</th> <th>shape</th> <th>efficient implementation</th> </tr> </thead> <tbody> <tr> <td>$\mathbf{q}_i$</td> <td>$QW^Q_i$</td> <td>$(B, l_x, d_k)$</td> <td>$(B, h, l_x, d_k)$</td> </tr> <tr> <td>$\mathbf{k}_i$</td> <td>$KW^K_i$</td> <td>$(B, l_z, d_k)$</td> <td>$(B, h, l_z, d_k)$</td> </tr> <tr> <td>$\mathbf{v}_i$</td> <td>$VW^V_i$</td> <td>$(B, l_z, d_v)$</td> <td>$(B, h, l_z, d_v)$</td> </tr> </tbody> </table> <p>Note that, for efficient implementation, we calculate $\mathbf{q}_i, \mathbf{k}_i, \mathbf{v}_i$ for all heads simulatenously.</p> <h3 id="scaled-dot-product-attention">Scaled dot product attention</h3> <p>Now, we can calculate attention score, not attention values, using the originally proposed formula</p> <p>\(\mathbf{S}_i(Q, K, V) = \text{softmax}\left(\frac{q_ik_i^T}{\sqrt{d_k}}\right)v_i\) Next we concatenate all the attention score to get the original dimension, \(\mathbf{S} \leftarrow [\mathbf{S}^1, \mathbf{S}^2, \dots, \mathbf{S}^h]\)</p> <blockquote> <p><strong><em>Implementation note:</em></strong> For efficiency, we can implement the heads simultaneously by reshapin the tensors, no need to concat later either</p> </blockquote> <p>Finally, the attention values would be the following, \(\mathbf{W}^O: \mathbf{S} \rightarrow \mathbf{Y}\) Shape of the input and output tensors would be the follwoing</p> <table> <thead> <tr> <th>tensor</th> <th>shape</th> </tr> </thead> <tbody> <tr> <td>$\mathbf{S}$</td> <td>$(B, l_x, h*d_v)$</td> </tr> <tr> <td>$\mathbf{Y}$</td> <td>$(B, l_x, d_\text{out})$</td> </tr> </tbody> </table> <h3 id="self-attention-cross-attention">Self attention, cross-attention</h3> <p>Depending on how we create $Q, K, V$ we can define two types of attention mechanism.</p> <ol> <li>self-attention: Same input, i.e. $\mathbf{X}$ or $\mathbf{Z}$ is used to represent all three matrices, so \(\begin{matrix} \mathbf{Q} = \mathbf{X} \\ \mathbf{K} = \mathbf{X} \\ \mathbf{V} = \mathbf{X} \end{matrix}\)</li> <li>cross-attention: input, $\mathbf{X}$ is used to represent the query, but output from another encoder, called $\text{memory}$, is used to represent key and value, so<br> \(\begin{matrix} \mathbf{Q} = \mathbf{X} \\ \mathbf{K} = \text{memory} \\ \mathbf{V} = \text{memory} \end{matrix}\)</li> </ol> <table> <thead> <tr> <th style="text-align: left">Algorithm 3: Multihead attention</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">input: $\mathbf{q} \in \mathbb{R}^{l_x \times d_k}, \mathbf{k} \in \mathbb{R}^{l_z\times d_k}, \mathbf{v} \in \mathbb{R}^{l_z\times d_v}, \mathbf{M}_{\mathbf{xz}} \in {0, 1}^{l_x\times l_z}: \text{Optional}$</td> </tr> <tr> <td style="text-align: left">1 $\mathbf{S} \leftarrow \mathbf{q}\mathbf{k}^T$</td> </tr> <tr> <td style="text-align: left">2 $\mathbf{S} \leftarrow \text{softmax}\left(\frac{\mathbf{S}}{d_k}\right)$</td> </tr> <tr> <td style="text-align: left">3 $\mathbf{S} \leftarrow \mathbf{S}\mathbf{v}$</td> </tr> <tr> <td style="text-align: left">4 $\mathbf{Y} \leftarrow \mathbf{W}^O \mathbf{S}$</td> </tr> </tbody> </table> <blockquote> <p><strong><em>Implementation note:</em></strong> For masking purpose, we can replace each masked element in $\mathbf{M}_\mathbf{xz}$ by $-\infty$ while making the non-masked elements as $0$s. In this way the softmax at line $2$ makes the masked element $0$ while only keeping the non-masked values</p> </blockquote> <h2 id="encoder">Encoder</h2> <p>Each encoder layer consists of two elements, 1) self-attention and 2) feedforward network (FFN)</p> <p>| Algorithm 4: Encoder Layer| | :— | | input: $\quad \mathbf{Z}: \text{encoder input}, \ \quad \text{class MultiheadAttention}, <br> \quad \mathbf{M}<em>{\mathbf{zz}}: \text{self-attention of encoder input}$| |1 $\text{for } k = 1, 2, \dots, h$ | |2 $\quad \mathbf{Z} \leftarrow \mathbf{Z} + \text{MultiheadAttention}(query=\mathbf{Z}, key=\mathbf{Z}, value=\mathbf{Z}, \mathbf{M}</em>{\mathbf{zz}}))$ | |3 $\quad \mathbf{Z} \leftarrow LN(\mathbf{Z})$|</p> <blockquote> <p><strong><em>Implementation note:</em></strong> As we are implementing the heads simultaneously, the loop is not really needed.</p> </blockquote> <h2 id="decoder">Decoder</h2> <p>Each decoder layer consists of three elements, 1) self-attention, 2) cross-attention and 3) feed forward network</p> <p>| Algorithm 5: Decoder layer| | :— | | input: $\quad \mathbf{X}: \text{decoder input}, \ \quad \text{memory}: \text{encoder output},\<br> \quad \text{class MultiheadAttention} \ \quad \mathbf{M}<em>{\mathbf{xx}}: \text{self-attention of decoder input},\ \quad \mathbf{M}</em>{\mathbf{xz}}: \text{cross attention of decoder input and encoder output}$| | 2 \ \ $\mathbf{X} = LN(\mathbf{X} + \text{MultiheadAttention}(query=\mathbf{X}, key=\mathbf{X}, value=\mathbf{X}, \mathbf{M}<em>{\mathbf{xx}})))$ | | 3 \ \ $\mathbf{X} = LN(\mathbf{X} + \text{MultiheadAttention}(query=\mathbf{X}, key=memory, value=memory, \mathbf{M}</em>{\mathbf{xz}}))$ | | 4 \ \ $\mathbf{X} = LN(\mathbf{X} + FFN(\mathbf{X}))$ |</p> <h2 id="bert-and-gpt">BERT and GPT</h2> <h3 id="encoder-transformer-bert">Encoder transformer (BERT)</h3> <p>Now, we can define the BERT model in a very straightforward fashion.</p> <p>| Algorithm 6: Encoder Transformer| | :— | | input: $\quad \mathbf{X}, \text{ vector representations of primary sequence}, \<br> \quad L_{enc}, \text{ number of encoder layers, EncoderLayer class}, \ \quad \mathbf{M}<em>{\mathbf{xx}}, \text{ target mask}$| | output: $\quad \mathbf{X}, \text{ representation of primary sequence with multi-head attention} \ \text{ which can be used for downstream applications}$| | 1 $\text{for } i = 1, 2, \dots, L</em>{enc}$| | 2 $\quad \mathbf{X} \leftarrow \text{EncoderLayer}(\mathbf{X}, \mathbf{M}_{\mathbf{xx}}\equiv 1)$ |</p> <p>Similarly, the GPT model can be presented as the following pseudocode.</p> <h3 id="decoder-transformer-gpt">Decoder transformer (GPT)</h3> <p>| Algorithm 7: Decoder Transformer| | :— | | input: $\quad \mathbf{X}, \text{vector representations of primary sequence}, <br> \quad L_{dec}, \text{number of decoder layers}, \<br> \quad \text{class} \text{ DecoderLayer}, <br> \quad \mathbf{M}<em>{\mathbf{xx}}, \text{target mask}$| | output: $\quad \mathbf{X}, \text{ representation of primary sequence with multi-head attention} \ \text{which can be used for downstream applications}$| | 1 For $i = 1, 2, \dots, L</em>{dec}$| | 2 $\quad \mathbf{X} \leftarrow \text{DecoderLayer}(\mathbf{X}, \mathbf{M}_{\mathbf{xx}}[t, t’] = [[t’\geq t]])$ |</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/images/">RL 0.0 Reinforcement learning primer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/welcome/">Why graduate student perspective?</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ferdous Alam. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>