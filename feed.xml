<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ferdous-alam.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ferdous-alam.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-02T08:30:54+00:00</updated><id>https://ferdous-alam.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Transformer implementation deconstructed</title><link href="https://ferdous-alam.github.io/blog/2022/formatting-and-links/" rel="alternate" type="text/html" title="Transformer implementation deconstructed"/><published>2022-08-21T16:40:16+00:00</published><updated>2022-08-21T16:40:16+00:00</updated><id>https://ferdous-alam.github.io/blog/2022/formatting-and-links</id><content type="html" xml:base="https://ferdous-alam.github.io/blog/2022/formatting-and-links/"><![CDATA[<blockquote> <p><strong><em>NOTE:</em></strong> Code implementation can be found at this <a href="https://github.com/ferdous-alam/DeconstructedML/tree/master/Deconstructed_Deep_learning/Transformers">repo</a>.</p> </blockquote> <h2 id="introduction">Introduction</h2> <p>Few symbols:</p> <table> <thead> <tr> <th>symbol</th> <th>tensor shape</th> <th>description</th> </tr> </thead> <tbody> <tr> <td>$B$</td> <td>-</td> <td>batch size</td> </tr> <tr> <td>$l_z$</td> <td>-</td> <td>context/source sequence length</td> </tr> <tr> <td>$l_x$</td> <td>-</td> <td>primary/target sequence length</td> </tr> <tr> <td>$d_\text{model}$</td> <td>-</td> <td>input dimension</td> </tr> <tr> <td>$d_k$</td> <td>-</td> <td>dimension of query/key embedding</td> </tr> <tr> <td>$d_v$</td> <td>-</td> <td>dimension of value embedding</td> </tr> <tr> <td>$B$</td> <td>-</td> <td>batch size</td> </tr> <tr> <td>$h$</td> <td>-</td> <td>number of heads</td> </tr> <tr> <td>$LN$</td> <td>-</td> <td>layer norm</td> </tr> <tr> <td>$FFN$</td> <td>-</td> <td>feed forward network</td> </tr> <tr> <td>$\mathbf{Z}$</td> <td>$(B, l_z, d_\text{model})$</td> <td>context/source sequence</td> </tr> <tr> <td>$\mathbf{X}$</td> <td>$(B, l_x, d_\text{model})$</td> <td>primary/target sequence</td> </tr> <tr> <td>$\mathbf{M}_{\mathbf{zz}}$</td> <td>$(B, l_z, l_z)$</td> <td>source mask</td> </tr> <tr> <td>$\mathbf{M}_{\mathbf{xx}}$</td> <td>$(B, l_x, l_x)$</td> <td>target mask</td> </tr> <tr> <td>$\mathbf{M}_{\mathbf{xz}}$</td> <td>$(l_z, l_z)$</td> <td>memory mask</td> </tr> <tr> <td>$L_{enc}$</td> <td>-</td> <td>number of encoder layers</td> </tr> <tr> <td>$L_{dec}$</td> <td>-</td> <td>number of decoder layers</td> </tr> </tbody> </table> <p>We start with the originally proposed encoder-decoder (ED) transformer [@vaswani2017attention]. If we only use the encoder of the transformer, then it is similar to the BERT [@devlin2018bert] model and if we only use the decoder of the transformer then it is similar to the GPT model [@radford2018improving]. For clarity, we consider a batched source sequence data $\mathbf{Z}$ which consists of $B$ sequences. Each sequence is of length $l_z$. This means that the sequence consists of $l_z$ number of tokens or vector representation of some input.</p> \[\mathbf{Z} = \begin{bmatrix} \mathbf{z}^1_1 &amp; \mathbf{z}^1_2 &amp; \dots &amp; \mathbf{z}^1_{l_z} \\ \mathbf{z}^2_1 &amp; \mathbf{z}^2_2 &amp; \dots &amp; \mathbf{z}^2_{l_z} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ \mathbf{z}^{B}_1 &amp; \mathbf{z}^{B}_2 &amp; \dots &amp; \mathbf{z}^{B}_{l_z} \end{bmatrix}\] <p>where, \(\mathbf{z}_i \in \mathbb{R}^{1\times d_\text{model}}, \ \ \ \ \ i = 1, 2, \dots, l_z\)</p> <p>Similarly, we consider a batched target sequence data $\mathbf{X}$ that contain $B$ sequences in total. Each target sequence is of length $l_x$.</p> \[\mathbf{X} = \begin{bmatrix} \mathbf{x}^1_1 &amp; \mathbf{x}^1_2 &amp; \dots &amp; \mathbf{x}^1_{l_x} \\ \mathbf{x}^2_1 &amp; \mathbf{x}^2_2 &amp; \dots &amp; \mathbf{x}^2_{l_x} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ \mathbf{x}^{B}_1 &amp; \mathbf{x}^{B}_2 &amp; \dots &amp; \mathbf{x}^{B}_{l_x} \end{bmatrix}\] <p>where, \(\mathbf{x}_i \in \mathbb{R}^{1\times d_\text{model}}, \ \ \ \ \ i = 1, 2, \dots, l_x\)</p> <p>The goal is to learn a representation of the target sequence, $\mathbf{X}$, that utilizes multi-head attention to capture important correlation within the sequence. Finally, people use these representations for various downstream tasks i.e. machine translation [@vaswani2017attention], next word prediction [@yang2019xlnet], computer vision [@dosovitskiy2020image], reinforcement learning [@chen2021decision] etc. A wonderful paper from DeepMind describes the formal algorithms for transformers [@phuong2022formal] very neatly.</p> <h3 id="mask">Mask</h3> <p>Mask allows the transformer to decide which part of the output should the model see at each timestep.</p> \[\mathbf{M}_{\mathbf{x}\mathbf{z}} = \begin{bmatrix} \text{Mask}[\mathbf{x}_0, \mathbf{z}_0] &amp; \text{Mask}[\mathbf{x}_0, \mathbf{z}_1]&amp; \text{Mask}[\mathbf{x}_0, \mathbf{z}_2] &amp; \dots &amp;\text{Mask}[\mathbf{x}_0, \mathbf{z}_{l_z}] \\ \text{Mask}[\mathbf{x}_1, \mathbf{z}_0] &amp; \text{Mask}[\mathbf{x}_1, \mathbf{z}_1]&amp; \text{Mask}[\mathbf{x}_1, \mathbf{z}_2] &amp; \dots &amp; \text{Mask}[\mathbf{x}_1, \mathbf{z}_{l_z}] \\ \vdots &amp; \vdots&amp; \vdots &amp; \dots &amp; \vdots \\ \text{Mask}[\mathbf{x}_{l_x}, \mathbf{z}_0] &amp; \text{Mask}[\mathbf{x}_{l_x}, \mathbf{z}_1]&amp; \text{Mask}[\mathbf{x}_{l_x}, \mathbf{z}_2] &amp; \dots &amp; \text{Mask}[\mathbf{x}_{l_x}, \mathbf{z}_{l_z}] \end{bmatrix} \in \mathbb{R}^{l_x \times l_z}\] <p>For example, if we want the model to see the whole sequence to calculate attention while training,then we do not need to do any masking. This model deploys <strong>bidirectional</strong> attention. The whole sequence should be available at the same time. For a single $i$-th sequence, the mask would look like this \(\mathbf{M}_{\mathbf{x}\mathbf{z}} = \begin{bmatrix} 1&amp; 1&amp; 1 &amp; \dots &amp; 1 \\ 1&amp; 1&amp; 1 &amp; \dots &amp; 1 \\ \vdots &amp; \vdots&amp; \vdots &amp; \dots &amp; \vdots \\ 1&amp; 1&amp; 1 &amp; \dots &amp; 1 \end{bmatrix} \in \mathbb{R}^{l_x \times l_z}\)</p> <p>Similarly, for auto-regressive models, we want the model to calculate attention based on unseen outputs until that timestep. Hence, the mask would be \(\mathbf{M}_{\mathbf{x}\mathbf{z}} = \begin{bmatrix} 1&amp; 0&amp; 0 &amp; \dots &amp; 0 \\ 1&amp; 1&amp; 0 &amp; \dots &amp; 0 \\ \vdots &amp; \vdots&amp; \vdots &amp; \dots &amp; \vdots \\ 1&amp; 1&amp; 1 &amp; \dots &amp; 1 \end{bmatrix} \in \mathbb{R}^{l_x \times l_z}\)</p> <p>In summary, if the length of each sequence at timestep $t$ is $t_x$ and $t_z$ respectively, then we can express each mask as the following,</p> \[\text{Mask}[t_x, t_z] = \begin{cases} 1 \quad \quad \quad \quad \text{for bidirectional attention} \\ [[t_x \geq t_z]] \quad \text{for unidirectional attention} \end{cases}\] <p>For conveniece, we introduce two additional terms:</p> <p>1) self-masks, $\mathbf{M}<em>{\mathbf{x}\mathbf{x}}$ or $\mathbf{M}</em>{\mathbf{z}\mathbf{z}}$: When we want to mask the same sequence against itself, for example we would use this sort of masking in the encoder part 2) cross-masks, $\mathbf{M}_{\mathbf{x}\mathbf{z}}$: When we want to mask a target sequence against a source sequence, for example we would use this sort of masking in the decoder part</p> <h2 id="formal-algorithm">Formal algorithm</h2> <p>First we provide a pseudoode of the encoder-decoder transformer algorithm. The following pseudocode is a simplified version of the formal algorithm presented in this paper [@phuong2022formal]. Initially I wanted to include the original pseudocode from the paper. But it seems like handling a lot of notations while thinking of the implementation. So, I added some trivial abstraction on top of that so the implementation becomes more convenient. Also, each matrix in that paper is transposed which makes the batched implementation little bit difficult to understand. So, I made some required modifications. This may reduce the technical correctness of the pseudocode, but I think that can be thought of as a simplification for implementation purpose.</p> <h3 id="encoder-decoder-transformer">Encoder-decoder transformer</h3> <p>| Algorithm 1: Encoder Decoder Transformer| | :— | | input: $\mathbf{Z} \in \mathbb{R}^{l_z \times d_\text{model}}$, $\mathbf{X} \in \mathbb{R}^{l_x \times d_\text{model} }$, vector representations of context and primary sequence, $L_{enc}$, $L_{dec}$, number of encoder and decoder layers, EncoderLayer class, DecoderLayer class, source_mask, target_mask, memory_mask| | output: $\mathbf{X} \in \mathbb{R}^{l_x \times d_\text{model}}$, representation of primary sequence with multi-head attention which can be used for downstream applications| | 1 For $i = 1, 2, \dots, L_{enc}$| | 2 $\quad \mathbf{Z} \leftarrow \text{EncoderLayer}(\mathbf{Z}, \mathbf{M}<em>{\mathbf{zz}}: Optional)$ | | 3 $\text{memory} = \mathbf{Z}$| | 4 For $j = 1, 2, \dots, L</em>{dec}$| | 5 $\quad \mathbf{X} \leftarrow \text{DecoderLayer}(\mathbf{X}, \text{memory}, \mathbf{M}<em>{\mathbf{xx}}: Optional, \mathbf{M}</em>{\mathbf{zz}}: Optional, \mathbf{M}_{\mathbf{xz}}: Optional)$|</p> <blockquote> <p><strong><em>Implementation note:</em></strong> For efficiency, deepcopy of a single encoder and decoder layer can be performed $L_{enc}$ and $L_{dec}$ times</p> </blockquote> <p>The training procedure is fairly simple and basically same as other neural network models. To make a broaded sense, I am not including any NLP specific output from the model. So, the output from the model is what we are interested in. We will also need to supply the target so that loss can be caluculated using the output and the target values. Finally we perform gradient descent to minimize the loss.</p> <h3 id="training-procedure">Training procedure</h3> <p>| Algorithm 2: Training Transformer| | :—————— | | $\text{input: } \text{class EDTransformer, class loss_func, learning rate $\eta$}$ | | 1 $\text{for } i = 1, 2, \dots, N_\text{epochs}$| | 2 $\quad \text{for } (\mathbf{Z}, \mathbf{X}, \text{target}) \text{ in train_dataloader } \quad \quad \text{# typical data loader for training data}$| | 3 $\quad \quad output \leftarrow \text{EDTransformer}(\mathbf{Z}, \mathbf{X}, \mathbf{M}<em>{\mathbf{xx}}: Optional, \mathbf{M}</em>{\mathbf{zz}}: Optional, \mathbf{M}_{\mathbf{xz}}: Optional)$| | 4 $\quad \quad \mathcal{L}(\mathbf{\theta}) = \text{loss_func}(\text{output, target})$| | 5 $\quad \quad \theta \leftarrow \theta - \eta \cdot \nabla\mathcal{L}(\theta)$| | 6 $\text{return } \mathbf{\theta}$|</p> <h2 id="attention">Attention</h2> <p>Transformers use multi-head attention to learn the contextual information of a sequence.</p> <h3 id="multihead-attention">Multihead attention</h3> <p>The original $Q, K, V$ matrices are projected into $h$ smaller matrices of using parameter matrices $W_i^Q, W_i^K, W_i^V$. Then attention is calculated for all these smaller matrices and concatened again to calculate attention for the full size input.</p> <table> <thead> <tr> <th>argument</th> <th>tensor shape</th> </tr> </thead> <tbody> <tr> <td>query $Q$</td> <td>$(B, l_x, d_\text{model})$</td> </tr> <tr> <td>key, $K$</td> <td>$(B, l_z, d_\text{model})$</td> </tr> <tr> <td>value, $V$</td> <td>$(B, l_z, d_\text{model})$</td> </tr> <tr> <td>$\text{multi_attn}$</td> <td>$(B, l_x, d_\text{out})$</td> </tr> </tbody> </table> <p>Let’s recall the original definition of multi-head attention: \(\text{Multi-head attention, }\mathbf{Y}(Q, K, V) = [\mathbf{S}_1; \dots; \mathbf{S}_h]W^O\) where $\mathbf{S}_i$ is the $i$-th single head attention score.</p> <p>As we are dividing the original $Q, K, V$ matrices into smaller matrices, dimension of $Q, K, V$ must be divisible by the number of heads, $h$. This is one way to do that if we want to divide $Q, K, V$ into the same dimension of smaller matrices. \(d_k = d_v = d_\text{model} / h\) Alternatively, we can divide $Q, K, V$ into different dimensions of smaller matrices as long as they match the original dimension.</p> <table> <thead> <tr> <th>Parameters</th> <th>dimension</th> </tr> </thead> <tbody> <tr> <td>query projection FFN, $W^Q$</td> <td>$\mathbb{R}^{d_\text{model}\times d_k}$</td> </tr> <tr> <td>key projection FFN, $W^K$</td> <td>$\mathbb{R}^{d_\text{model}\times d_k}$</td> </tr> <tr> <td>value projection FFN, $W^V$</td> <td>$\mathbb{R}^{d_\text{out}\times d_v}$</td> </tr> <tr> <td>output projection FFN, $W^O$</td> <td>$\mathbb{R}^{hd_v\times d_\text{out}}$</td> </tr> </tbody> </table> <p>So, these parameter weight matrices help to project the original $Q, K, V$ into smaller $h$ number of $q, k, v$ matrices for multi-head purpose. \(\text{query projection, }W_i^Q: Q \rightarrow \mathbf{q}_i\) \(\text{key projection, }W_i^K: K \rightarrow \mathbf{k}_i\) \(\text{value projection, }W_i^V: V \rightarrow \mathbf{v}_i\) For efficient implementation we calculate the attention score for all heads simultaneously by reshaping the tensors. So, the shape of the smaller tensors end up being the following,</p> <table> <thead> <tr> <th>tensor</th> <th>expression</th> <th>shape</th> <th>efficient implementation</th> </tr> </thead> <tbody> <tr> <td>$\mathbf{q}_i$</td> <td>$QW^Q_i$</td> <td>$(B, l_x, d_k)$</td> <td>$(B, h, l_x, d_k)$</td> </tr> <tr> <td>$\mathbf{k}_i$</td> <td>$KW^K_i$</td> <td>$(B, l_z, d_k)$</td> <td>$(B, h, l_z, d_k)$</td> </tr> <tr> <td>$\mathbf{v}_i$</td> <td>$VW^V_i$</td> <td>$(B, l_z, d_v)$</td> <td>$(B, h, l_z, d_v)$</td> </tr> </tbody> </table> <p>Note that, for efficient implementation, we calculate $\mathbf{q}_i, \mathbf{k}_i, \mathbf{v}_i$ for all heads simulatenously.</p> <h3 id="scaled-dot-product-attention">Scaled dot product attention</h3> <p>Now, we can calculate attention score, not attention values, using the originally proposed formula</p> <p>\(\mathbf{S}_i(Q, K, V) = \text{softmax}\left(\frac{q_ik_i^T}{\sqrt{d_k}}\right)v_i\) Next we concatenate all the attention score to get the original dimension, \(\mathbf{S} \leftarrow [\mathbf{S}^1, \mathbf{S}^2, \dots, \mathbf{S}^h]\)</p> <blockquote> <p><strong><em>Implementation note:</em></strong> For efficiency, we can implement the heads simultaneously by reshapin the tensors, no need to concat later either</p> </blockquote> <p>Finally, the attention values would be the following, \(\mathbf{W}^O: \mathbf{S} \rightarrow \mathbf{Y}\) Shape of the input and output tensors would be the follwoing</p> <table> <thead> <tr> <th>tensor</th> <th>shape</th> </tr> </thead> <tbody> <tr> <td>$\mathbf{S}$</td> <td>$(B, l_x, h*d_v)$</td> </tr> <tr> <td>$\mathbf{Y}$</td> <td>$(B, l_x, d_\text{out})$</td> </tr> </tbody> </table> <h3 id="self-attention-cross-attention">Self attention, cross-attention</h3> <p>Depending on how we create $Q, K, V$ we can define two types of attention mechanism.</p> <ol> <li>self-attention: Same input, i.e. $\mathbf{X}$ or $\mathbf{Z}$ is used to represent all three matrices, so \(\begin{matrix} \mathbf{Q} = \mathbf{X} \\ \mathbf{K} = \mathbf{X} \\ \mathbf{V} = \mathbf{X} \end{matrix}\)</li> <li>cross-attention: input, $\mathbf{X}$ is used to represent the query, but output from another encoder, called $\text{memory}$, is used to represent key and value, so<br/> \(\begin{matrix} \mathbf{Q} = \mathbf{X} \\ \mathbf{K} = \text{memory} \\ \mathbf{V} = \text{memory} \end{matrix}\)</li> </ol> <table> <thead> <tr> <th style="text-align: left">Algorithm 3: Multihead attention</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">input: $\mathbf{q} \in \mathbb{R}^{l_x \times d_k}, \mathbf{k} \in \mathbb{R}^{l_z\times d_k}, \mathbf{v} \in \mathbb{R}^{l_z\times d_v}, \mathbf{M}_{\mathbf{xz}} \in {0, 1}^{l_x\times l_z}: \text{Optional}$</td> </tr> <tr> <td style="text-align: left">1 $\mathbf{S} \leftarrow \mathbf{q}\mathbf{k}^T$</td> </tr> <tr> <td style="text-align: left">2 $\mathbf{S} \leftarrow \text{softmax}\left(\frac{\mathbf{S}}{d_k}\right)$</td> </tr> <tr> <td style="text-align: left">3 $\mathbf{S} \leftarrow \mathbf{S}\mathbf{v}$</td> </tr> <tr> <td style="text-align: left">4 $\mathbf{Y} \leftarrow \mathbf{W}^O \mathbf{S}$</td> </tr> </tbody> </table> <blockquote> <p><strong><em>Implementation note:</em></strong> For masking purpose, we can replace each masked element in $\mathbf{M}_\mathbf{xz}$ by $-\infty$ while making the non-masked elements as $0$s. In this way the softmax at line $2$ makes the masked element $0$ while only keeping the non-masked values</p> </blockquote> <h2 id="encoder">Encoder</h2> <p>Each encoder layer consists of two elements, 1) self-attention and 2) feedforward network (FFN)</p> <p>| Algorithm 4: Encoder Layer| | :— | | input: $\quad \mathbf{Z}: \text{encoder input}, \ \quad \text{class MultiheadAttention}, <br/> \quad \mathbf{M}<em>{\mathbf{zz}}: \text{self-attention of encoder input}$| |1 $\text{for } k = 1, 2, \dots, h$ | |2 $\quad \mathbf{Z} \leftarrow \mathbf{Z} + \text{MultiheadAttention}(query=\mathbf{Z}, key=\mathbf{Z}, value=\mathbf{Z}, \mathbf{M}</em>{\mathbf{zz}}))$ | |3 $\quad \mathbf{Z} \leftarrow LN(\mathbf{Z})$|</p> <blockquote> <p><strong><em>Implementation note:</em></strong> As we are implementing the heads simultaneously, the loop is not really needed.</p> </blockquote> <h2 id="decoder">Decoder</h2> <p>Each decoder layer consists of three elements, 1) self-attention, 2) cross-attention and 3) feed forward network</p> <p>| Algorithm 5: Decoder layer| | :— | | input: $\quad \mathbf{X}: \text{decoder input}, \ \quad \text{memory}: \text{encoder output},\<br/> \quad \text{class MultiheadAttention} \ \quad \mathbf{M}<em>{\mathbf{xx}}: \text{self-attention of decoder input},\ \quad \mathbf{M}</em>{\mathbf{xz}}: \text{cross attention of decoder input and encoder output}$| | 2 \ \ $\mathbf{X} = LN(\mathbf{X} + \text{MultiheadAttention}(query=\mathbf{X}, key=\mathbf{X}, value=\mathbf{X}, \mathbf{M}<em>{\mathbf{xx}})))$ | | 3 \ \ $\mathbf{X} = LN(\mathbf{X} + \text{MultiheadAttention}(query=\mathbf{X}, key=memory, value=memory, \mathbf{M}</em>{\mathbf{xz}}))$ | | 4 \ \ $\mathbf{X} = LN(\mathbf{X} + FFN(\mathbf{X}))$ |</p> <h2 id="bert-and-gpt">BERT and GPT</h2> <h3 id="encoder-transformer-bert">Encoder transformer (BERT)</h3> <p>Now, we can define the BERT model in a very straightforward fashion.</p> <p>| Algorithm 6: Encoder Transformer| | :— | | input: $\quad \mathbf{X}, \text{ vector representations of primary sequence}, \<br/> \quad L_{enc}, \text{ number of encoder layers, EncoderLayer class}, \ \quad \mathbf{M}<em>{\mathbf{xx}}, \text{ target mask}$| | output: $\quad \mathbf{X}, \text{ representation of primary sequence with multi-head attention} \ \text{ which can be used for downstream applications}$| | 1 $\text{for } i = 1, 2, \dots, L</em>{enc}$| | 2 $\quad \mathbf{X} \leftarrow \text{EncoderLayer}(\mathbf{X}, \mathbf{M}_{\mathbf{xx}}\equiv 1)$ |</p> <p>Similarly, the GPT model can be presented as the following pseudocode.</p> <h3 id="decoder-transformer-gpt">Decoder transformer (GPT)</h3> <p>| Algorithm 7: Decoder Transformer| | :— | | input: $\quad \mathbf{X}, \text{vector representations of primary sequence}, <br/> \quad L_{dec}, \text{number of decoder layers}, \<br/> \quad \text{class} \text{ DecoderLayer}, <br/> \quad \mathbf{M}<em>{\mathbf{xx}}, \text{target mask}$| | output: $\quad \mathbf{X}, \text{ representation of primary sequence with multi-head attention} \ \text{which can be used for downstream applications}$| | 1 For $i = 1, 2, \dots, L</em>{dec}$| | 2 $\quad \mathbf{X} \leftarrow \text{DecoderLayer}(\mathbf{X}, \mathbf{M}_{\mathbf{xx}}[t, t’] = [[t’\geq t]])$ |</p>]]></content><author><name></name></author><category term="generative-models"/><category term="AI,"/><category term="Transformer"/><summary type="html"><![CDATA[A cheat sheet for implementing transformers.]]></summary></entry><entry><title type="html">RL 0.0 Reinforcement learning primer</title><link href="https://ferdous-alam.github.io/blog/2021/images/" rel="alternate" type="text/html" title="RL 0.0 Reinforcement learning primer"/><published>2021-09-25T21:01:00+00:00</published><updated>2021-09-25T21:01:00+00:00</updated><id>https://ferdous-alam.github.io/blog/2021/images</id><content type="html" xml:base="https://ferdous-alam.github.io/blog/2021/images/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">{r setup, include=FALSE} knitr::opts_chunk$set(echo = FALSE) </code></p> <h2 id="introduction">Introduction</h2> <p>Here I will try to explain how RL stems from the sequential decision making framework and its close relation with optimal control theory. I will follow two primary references, reinforcement learning and optimal control [@bertsekas2019reinforcement] and introduction to reinforcement learning [@sutton2018reinforcement]</p> <h2 id="online-sequential-decision-making">Online sequential decision making</h2> <p>The goal is to take sequential decisions “online” to achieve a certain goal; often times it is maximizing a performance objective which can be thougt of as a function $J(\cdot)$. The input to this objective function is not important right now. Let’s call this decision maker “agent”. The catch is that the agent has to figure out which decision to take based on the observed feedback from the envrionment of its interest. To observe a feedback the agent has to interact with the envrionment through some sort of actions. So, optimization will be at the core of this decision making procedure while we use data collected in an online fashion to identify actions to take. This is why the “learning” happens.</p> <h2 id="bandit-a-mandatory-prior-to-rl">Bandit: A mandatory prior to RL</h2> <p>Consider an online sequential decision making problem where an agent has $k$ choices to choose an action and everytime it executes an action it receives a feedback from the environment. A fundamental question then aries for the agent: how to choose an action? The way it chooses an action describes its way of behaving in this particular environment which is known as the “policy” denoted as $\pi$. Note that $\pi$ decribes how to take an action but it does not say how to take the best action that will maximize the performance objective $J(\cdot)$. To identify the optimal action we need to find out the optimal policy $\pi^<em>$. So, the following makes sense \begin{equation} \pi^</em> = \text{argmax}_\pi J^\pi(\cdot) \end{equation} where $J^\pi(\cdot)$ is the value of the performance objective obtained using policy $\pi$. Note that the agent does not know the underlying distribution of the feedback from each action it takes. If it were known then the agent could easily pick the best action. This setting is known as the <em>bandit</em> problem or sometimes as “multi-armed bandit (MAB)” problem. Sometimes people call this “k-armed bandit” as well. Usually the feedback obtained from the environment is known as <strong>reward</strong> or <strong>cost</strong>.</p> <h2 id="markov-decision-process-the-rl-formalism">Markov Decision Process: The RL formalism</h2> <p>Until now it is clear that we are interested in sequential decision making. To formalize such process we will adopt the <strong>`Markov Decision Process (MDP)’</strong>. An MDP $\mathcal{M}$ is usually expressed as a tuple of these following 5-elements. \(\mathcal{M} = \langle \mathcal{X}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \gamma\rangle\) where,</p> <ul> <li>$\mathcal{X}$ is the state-space, a set of states, $\mathbf{x}\in\mathcal{X}$</li> <li>$\mathcal{A}$ is the action-space, a set of actions, $a\in\mathcal{X}$</li> <li>$\mathcal{R}$ is the reward function, usually defined in the product space, $\mathcal{R}:\mathcal{X}\times\mathcal{A}\rightarrow \mathbb{R}$</li> <li> <table> <tbody> <tr> <td>$\mathcal{P}$ is the transition probability function, also known as the dynamics of the system, that describes the conditional probability $p(\mathbf{x}_{t+1}</td> <td>\mathbf{x}_t, a_t)$</td> </tr> </tbody> </table> </li> <li>$\gamma$ is a discount factor, $\gamma \in [0, 1]$</li> </ul> <p>Let’s focus on how this formalism helps in sequential decision making. Assume that the agent is in a current state $\mathbf{x}<em>t$ at timestep $t$. Based on some policy $\pi$ it takes a decision to move to state $\mathbf{x}</em>{t+1}$ by taking action $a_t$. To move to that state, the agent needs to know the probability of moving to that state given the current state $\mathbf{x}<em>t$ and action $a_t$. This is how the conditional probability comes into the process. Once the agent reaches state $\mathbf{x}</em>{t+1}$ it gets a feedback from the environment. This feedback is called a reward value, $R_t$, which is usually a scalar numeric value. We assume that the reward value $R_t$ comes as the output from the reward function $\mathcal{R}$ while it takes $\mathbf{x}<em>t$ and $a_t$ as input, meaning $\mathcal{R}(\mathbf{x}_t, a_t): \mathcal{X} \times \mathcal{A} \rightarrow R_t$. What if the reward function only depends on the current state and not the action? Then the reward function would be represented as $\mathcal{R}(\mathbf{x}_t): \mathcal{X} \rightarrow R_t$. Similary if the reward depends on not only the current state and current action but also the state it ends up in, then we would use the description of the reward function as $\mathcal{R}(\mathbf{x}_t, a_t, \mathbf{x}</em>{t+1}): \mathcal{X} \times \mathcal{A} \times \mathcal{X} \rightarrow R_t$. Finally the agent uses a discount factor $\gamma$ to put less weight onto future rewards and more weight into recent rewards. This makes sense because the agent does not want to depend strongly on the information that comes after many timesteps into the future. All these information can be combined very convenienty in an MDP. Now it should be easier to follow why MDP is attractive for sequential decision making.</p> <p>This formalism is great, but what is the goal of the agent in an MDP? In simplified terms the ‘goal’ of the agent is to maximize the accumulation of rewards. Let’s define the accumulation of rewards as <strong>return</strong>. The return obtained at timestep $t$ can be expressed as $G_t^\pi = R_t + R_{t+1} + R_{t+2} + \dots$. This means that if the agent moves to state $\mathbf{x}<em>{t+1}$ from state $\mathbf{x}_t$ by taking action $a_t$ it receives reward $R_t$ and at the next timestep if it follows the same policy it will receive reward $R</em>{t+1}$ and so on. For an infinite horizon case, the return will blow up. This is why we use a discount factor $\gamma$ such that \(G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots = \sum_{t=0}^{\infty} \gamma^k R_{t+k}.\)</p> <p>This discount factor serves two purposes: a) it provides more weight into recent rewards and b) it helps to keep the return as a finite value. If we do not use a discount factor in the MDP definition then those MDPs are called ‘undiscounted MDPs’.</p> <p>But how do we maximize the return? The answer is pretty simple: by choosing the sequence of actions that provides the highest return. These actions are called ‘optimal actions’. Remember that actions are chosen according to a policy $\pi:\mathcal{X} \rightarrow \mathcal{A}$. So to choose the optimal actions we need an optimal policy $\pi^*$. Now we have successfully identified the fundamental goal in this learning scheme: “how to obtian the optimal policy for sequential decision making?”</p> <p>To identify whether a state is good or bad we need to assign some sort of value to that state. Usually this is known as the <strong>value function</strong>. The agent would like to explore states which have higher values compared to the rest. To derive the value of a state we use the reward function in an intuitive way. Let’s take a look. \(V^\pi(\mathbf{x}) = \mathbb{E}^\pi[G_t|\mathbf{x}] = \mathbb{E}^\pi \left[R_{t} + \gamma R_{t+1} + \dots |\mathbf{x}\right] = \mathbb{E}^\pi\left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k} |\mathbf{x}\right]\) Here we take the ‘expectation’ of the return to account for the stochasticity of the rewards. Notice that we are only taking into consideration the mean value of the return, not the variance. This sometimes cause a variance issue in developed algorithms based on this formalism. So, what does $V^\pi(\mathbf{x})$ mean? This means that the value of a state while following a policy $\pi$ is the expected value of the return. Let’s develop a simple algorithm that can help us figure out the optimal policy $\pi^*$ using the value of the states.</p> <table> <thead> <tr> <th style="text-align: left">Algorithm 1</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">1 Find value of all states, $V^\pi(\mathbf{x})$ where $\mathbf{x}\in\mathcal{X}$</td> </tr> <tr> <td style="text-align: left">2 From each state find the next best state $\mathbf{x}<em>b = \text{argmax}</em>{\mathbf{x}’} V^\pi(\mathbf{x}’)$</td> </tr> <tr> <td style="text-align: left">3 Find the optimal policy by choosing the action that led to $\mathbf{x}_b$ meaning $\pi^*(\mathbf{x}) = {a: \mathbf{x} \rightarrow \mathbf{x}_b}$</td> </tr> </tbody> </table> <p>Would not it be better if we could, rather than finding the value of a state, directly find the value of an action from a state? In that way we would be able to evaluate whether an action is good or bad based on the assigned value. Yes, we can and this is known as the action-value functions. These are also known as <strong>Q-values</strong> as they can be informally thought of as the <strong>quality of an action taken from a state</strong>. For convenience, an action taken from a state is combinedly referred as the state-action, $(\mathbf{x}, a)$, pair. So, how do we define Q-values? Looking closely to the definition of the value-functions we can similarly define the Q-values by conditioning the return on the state-action pair.</p> <p>\(Q^\pi(\mathbf{x}, a) = \mathbb{E}^\pi[G_t| \mathbf{x}, a] = \mathbb{E}^\pi \left[R_{t} + \gamma R_{t+1} + \dots |\mathbf{x}, a\right] = \mathbb{E}^\pi\left[ \sum_{k=0}^{\infty}\gamma^k R_{t+k}|\mathbf{x}, a\right]\) So, Q-values are the values assigned to the state-action pair and values are assigned to the states only. Can we derive any relationship between them based on their properties? To do that we need to break down their formal definition using the properties of the expectation operator in the above equations. Let’s break down the equation further using the definition of an expectation. We will use the following three properties of the expectation operator.</p> <div style="background-color:rgba(0, 0, 0, 0.0470588);"> <details> <summary>Expectation of a random variable</summary> <p> 1. $p1$: Remember that if $X$ is a discrete random variable with finite number of outcomes $x_1, x_2, \dots, x_k$ with probabilities $p_1, p_2, \dots, p_k$ then $$\begin{equation} \mathbb{E}[X] = p_1x_1 + p_2x_2 + \dots + p_kx_k\end{equation}$$ 2. $p2$: Expectations are linear operator, meaning $$\mathbb{E}[X_1] + \mathbb{E}[X_2] = \mathbb{E}[X_1 + X_2]$$ 3. $p3$: For conditional expectations using partition theorem, $$\mathbb{E}[X] = \sum_y p(Y=y) \mathbb{E}[X|Y=y]$$ </p> </details> </div> <p>So, from the value function definition we get,</p> \[\begin{aligned} V^\pi(\mathbf{x}) &amp;= \mathbb{E}^\pi [R_t + \gamma G_{t+1}|\mathbf{x}] \\ &amp;= \sum_{a\in\mathcal{A}} \pi(a|\mathbf{x}) \mathbb{E}[R_t + \gamma G_{t+1}|\mathbf{x}, a] \ \ \ \ \text{ using } p3\\ &amp;= \sum_{a\in\mathcal{A}} \pi(a|\mathbf{x}) \sum_{\mathbf{x}'\in\mathcal{\mathcal{X}}} p(\mathbf{x}, a, \mathbf{x}') \mathbb{E}[R_t + \gamma G_{t+1}|\mathbf{x}, a, \mathbf{x}'] \ \ \ \ \text{ using } p3\\ &amp;= \sum_{a\in\mathcal{A}} \pi(a|\mathbf{x}) \sum_{\mathbf{x}'\in\mathcal{\mathcal{X}}} p(\mathbf{x}, a, \mathbf{x}') \left[\underbrace{\mathbb{E}[R_t]}_{r(\mathbf{x}, a)} + \mathbb{E}[\gamma G_{t+1}|\mathbf{x}, a, \mathbf{x}'] \right] \ \ \ \ \text{ using } p2\\ &amp;= \sum_{a\in\mathcal{A}} \pi(a|\mathbf{x}) \sum_{\mathbf{x}'\in\mathcal{\mathcal{X}}} p(\mathbf{x}, a, \mathbf{x}') \left[ r(\mathbf{x}, a) + \gamma \underbrace{\mathbb{E}[G_{t+1}|\mathbf{x}']}_{V^\pi(\mathbf{x}')} \right] \ \ \ \ \text{ using } p2\\ &amp;= \sum_{a\in\mathcal{A}} \pi(a|\mathbf{x}) \sum_{\mathbf{x}'\in\mathcal{\mathcal{X}}} p(\mathbf{x}, a, \mathbf{x}') \left[ r(\mathbf{x}, a) + \gamma V^\pi(\mathbf{x}') \right] \end{aligned}\] <p>This gives us a recursive formula! Similarly we can formulate the Q-values.</p> \[\begin{aligned} Q^\pi(\mathbf{x}, a) &amp;= \mathbb{E}^\pi\left[ R_t + \gamma G_{t+1} | \mathbf{x}, a\right]\\ &amp;= \mathbb{E}^\pi\left[ R_t + \gamma G_{t+1} | \mathbf{x}, a\right]\\ &amp;= \sum_{\mathbf{x}'}p(\mathbf{x}, a, \mathbf{x}') \mathbb{E}^\pi\left[ R_t + \gamma G_{t+1} | \mathbf{x}, a, \mathbf{x}'\right]\\ &amp;= \sum_{\mathbf{x}'}p(\mathbf{x}, a, \mathbf{x}') \left[ \mathbb{E}^\pi[R_t|\mathbf{x}, a, \mathbf{x}'] + \gamma \mathbb{E}^\pi \left[G_{t+1} | \mathbf{x}, a, \mathbf{x}'\right] \right]\\ &amp;= \sum_{\mathbf{x}'}p(\mathbf{x}, a, \mathbf{x}') \left[ r(\mathbf{x}, a) + \gamma \mathbb{E}^\pi \left[G_{t+1} | \mathbf{x}'\right] \right]\\ &amp;= \sum_{\mathbf{x}'}p(\mathbf{x}, a, \mathbf{x}') \left[ r(\mathbf{x}, a) + \gamma V^\pi(\mathbf{x}')\right] \end{aligned}\] <p>We can also develop relationship between the Q-values and value functions.</p> \[\begin{aligned} V^\pi(\mathbf{x}) &amp;= \mathbb{E}^\pi[G_t | \mathbf{x}]\\ &amp;= \sum_a \pi(a|\mathbf{x}) \mathbb{E}^\pi[G_t|\mathbf{x}, a]\\ &amp;= \sum_a \pi(a|\mathbf{x}) Q^\pi(\mathbf{x}, a) \end{aligned}\] <p>Finally we develop the recursive formula for Q-values.</p> <p>$$\begin{align} Is there any way to determine the value of all the states within the state-space? Yes, these algorithms are known as dynamic programming algorithms.</p> <h2 id="dynamic-programming">Dynamic programming</h2> <table> <thead> <tr> <th style="text-align: left">Algorithm 2: Policy iteration</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">1</td> </tr> <tr> <td style="text-align: left">2</td> </tr> <tr> <td style="text-align: left">3</td> </tr> <tr> <td style="text-align: left">4</td> </tr> <tr> <td style="text-align: left">5</td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: left">Algorithm 2: Value iteration</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">1</td> </tr> <tr> <td style="text-align: left">2</td> </tr> <tr> <td style="text-align: left">3</td> </tr> <tr> <td style="text-align: left">4</td> </tr> <tr> <td style="text-align: left">5</td> </tr> </tbody> </table> <p>Note that all the future rewards in the return equation is unknown. <strong>If</strong> we knew about all the possible future rewards we are going to get from current timestep and forward, then we could obtain the exact value of the return. But unfortunately we do not have the luxury to know all these reward values apriori. This is where <strong>Reinforcement Learning (RL)</strong> comes into play.</p> <h2 id="reinforcement-learning-and-optimal-control">Reinforcement learning and optimal control</h2> <h2 id="building-algorithms-for-rl">Building algorithms for RL</h2> <h2 id="types-of-rl-algorithms">Types of RL algorithms</h2> <h2 id="sample-complexity-of-rl-algorithms">Sample complexity of RL algorithms</h2> <h2 id="a-case-study">A case study</h2> <h2 id="conclusion">Conclusion</h2>]]></content><author><name></name></author><category term="fundamentals"/><category term="AI,"/><category term="RL"/><summary type="html"><![CDATA[A trivial attempt to unify the fundamental RL concepts in one place for building intuitions.]]></summary></entry><entry><title type="html">Why graduate student perspective?</title><link href="https://ferdous-alam.github.io/blog/2021/welcome/" rel="alternate" type="text/html" title="Why graduate student perspective?"/><published>2021-05-21T00:00:00+00:00</published><updated>2021-05-21T00:00:00+00:00</updated><id>https://ferdous-alam.github.io/blog/2021/welcome</id><content type="html" xml:base="https://ferdous-alam.github.io/blog/2021/welcome/"><![CDATA[<h2 id="why-graduate-student-perspective">Why graduate student perspective?</h2> <p>Machine learning, Reinforcement learning and control theory are vast! The stream of research articles are often overwhelming. Sometimes I like to take a step back and surprisingly I find peace exploring fundamental topics and explaining them. I remeber there was a time when I wanted a blog that explains complex topics in intuitive ways. Now-a-days the chaotic stream of online blogs have made it extremely difficult for me to find a reliable blog post quickly. I personally enjoy a nice balance between formal presentation of concepts (because they are robust!) followed by an interesting toy example for building intuitions. I like to call this approach <strong>Intuitive formalism</strong> or often during my PhD research presentations I use the term <strong>visual formalism</strong>. Unfortunately I can not find a lot of online blog posts/writings according to my taste. So I decided to make a blog for myself that gave birth to this site. In the unlikely case of you reading this I appreciate your time here. Cheers! :)</p>]]></content><author><name></name></author><category term="welcome"/><category term="welcome"/><summary type="html"><![CDATA[My motivation for starting this blog]]></summary></entry></feed>